import json
from tqdm import tqdm
from transformers import T5ForConditionalGeneration, AutoTokenizer, MT5ForConditionalGeneration, MBartForConditionalGeneration
from bert_score import score as bert_score
from sacrebleu.metrics import CHRF
from natasha import Segmenter, NewsEmbedding, NewsNERTagger, Doc
import torch
from transformers import pipeline

flag = False
device = torch.device("cuda" if torch.cuda.is_available() and flag else "cpu")

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è NER
segmenter = Segmenter()
emb = NewsEmbedding()
ner_tagger = NewsNERTagger(emb)

test_phrases = [
    "—á—Ç–æ –∑–∞ —Ñ–∏–≥–Ω—é —Ç—ã 8–≥–æ –Ω–∞ —Å–æ–≤–µ—â–∞–Ω–∏–∏ –ª—è–ø–Ω—É–ª? –ê –∫–∞–∫ –º–Ω–µ —Ç–æ –±—ã—Ç—å –µ—Å–ª–∏ –í–∞–Ω—è –Ω–∏—á–µ –Ω–µ –¥–µ–ª–∞–µ—Ç. –Ø –∑–∞–≤—Ç—Ä–∞ –Ω–µ –ø—Ä–∏–¥—É, "
    "—Å–∞–º —Ä–µ—à–∞–π –Ω–∞—à–∏ –∑–∞–¥–∞—á–∏.",
    "08.03 –Ω–µ –ø—Ä–∏–¥—É",
    "–Ω—É —á—Ç–æ –∑–∞ –∂–µ—Å—Ç—å –ø–æ—á–µ–º—É –æ–ø—è—Ç—å —É –≤–∞—Å –Ω–∏—á–µ–≥–æ –Ω–µ –ø–æ–ª—É—á–∞–µ—Ç—Å—è",
    "–ù–µ—Ç, —è –Ω–µ —Å–æ–±–∏—Ä–∞—é—Å—å –µ—Ö–∞—Ç—å. –ü—É—Å—Ç—å –∫—Ç–æ-—Ç–æ –¥—Ä—É–≥–æ–π —ç—Ç–∏–º –∑–∞–Ω–∏–º–∞–µ—Ç—Å—è.",
    "–í—Å—ë, –≤–æ–ø—Ä–æ—Å –∑–∞–∫—Ä—ã—Ç: –∏—Å–ø—ã—Ç–∞—Ç–µ–ª—å–Ω—ã–π —Å—Ä–æ–∫ —Ç—Ä–∏ –º–µ—Å—è—Ü–∞, –∏ –Ω–∏–∫—Ç–æ –Ω–µ —Å–¥–µ–ª–∞–µ—Ç –∏—Å–∫–ª—é—á–µ–Ω–∏–π.",
    "—Å–¥–µ–ª–∞–π—Ç–µ –ø–æ–∂–∞–ª—É—Å—Ç–∞ –∫–∞–∫ –º–æ–∂–Ω–æ –±—ã—Å—Ç—Ä–µ–µ",
    "–†–∞–±–æ—á–∏–π –¥–µ–Ω—å –∂–µ –∑–∞–∫–æ–Ω—á–∏–ª—Å—è, –∫–∞–∫–æ–≥–æ —Ö—Ä–µ–Ω–∞ —è –≤—Å—ë –µ—â—ë —Ç—É—Ç?",
    "–ì–µ–Ω–∏–∞–ª—å–Ω–∞—è –∏–Ω–∏—Ü–∏–∞—Ç–∏–≤–∞, –∫–∞–∫ –∏ –≤—Å–µ –ø—Ä–µ–¥—ã–¥—É—â–∏–µ ‚Äì –±–µ—Å–ø–æ–ª–µ–∑–Ω–∞—è –∏ –Ω–µ–Ω—É–∂–Ω–∞—è.",
    "–¢—ã –æ–ø—è—Ç—å –∑–∞–±—ã–ª –ø—Ä–æ —Å—Ä–æ–∫–∏ –¥–ª—è —Å–µ–º–∏–Ω–∞—Ä–∞? –≠—Ç–æ —É–∂–µ –Ω–∞—á–∏–Ω–∞–µ—Ç —Ä–∞–∑–¥—Ä–∞–∂–∞—Ç—å. –í—Å–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –≥–æ—Ç–æ–≤—ã –∑–∞ –¥–≤–∞ –¥–Ω—è, "
    "–∞ —É –Ω–∞—Å –¥–æ —Å–∏—Ö –ø–æ—Ä –Ω–µ—Ç –¥–∞–∂–µ —Å–ø–∏—Å–∫–∞ —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤. –ö–∞–∫ —Ç—ã –≤–æ–æ–±—â–µ –ø–ª–∞–Ω–∏—Ä—É–µ—à—å –≤—Å—ë —É—Å–ø–µ—Ç—å?",
    "–û—Ç–≤–∞–ª–∏"
]


def add_para(line):
    return 'paraphrase politely: ' + line


def extract_named_entities(text):
    doc = Doc(text)
    doc.segment(segmenter)
    doc.tag_ner(ner_tagger)
    return {span.text for span in doc.spans}


def evaluate_model(model, tokenizer, dataset_path, max_tokens=128, batch_size=8, mbart=False):
    # –ß—Ç–µ–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞ —Å –ø—Ä–æ–≥—Ä–µ—Å—Å–±–∞—Ä–æ–º
    data = []
    with open(dataset_path, "r", encoding="utf-8") as f:
        for line in tqdm(f, desc="Loading dataset"):
            data.append(json.loads(line))

    inputs = [d["input"] for d in data]
    references = [d["output"] for d in data]

    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å –±–∞—Ç—á–∏–Ω–≥–æ–º –∏ –ø—Ä–æ–≥—Ä–µ—Å—Å–±–∞—Ä–æ–º
    predictions = []
    for i in tqdm(range(0, len(inputs), batch_size), desc="Generating outputs (8 sent * 125 iter)"):
        batch_inputs = inputs[i:i+batch_size]
        inputs_tokenized = tokenizer(batch_inputs, return_tensors="pt", padding=True, truncation=True)
        inputs_tokenized = {k: v.to(device) for k, v in inputs_tokenized.items()}
        if mbart:
            with torch.no_grad():
                outputs = model.generate(
                    **inputs_tokenized,
                    forced_bos_token_id=tokenizer.lang_code_to_id[tokenizer.tgt_lang],
                    max_length=max_tokens,
                    num_beams=4,
                    do_sample=True,
                    temperature=1.0,
                )
            predictions += [tokenizer.decode(g, skip_special_tokens=True) for g in outputs]
        else:
            with torch.no_grad():
                generated_ids = model.generate(
                    input_ids=inputs_tokenized["input_ids"],
                    attention_mask=inputs_tokenized["attention_mask"],
                    max_new_tokens=max_tokens
                )
            for g in generated_ids:
                predictions.append(tokenizer.decode(g, skip_special_tokens=True))
    print(f"–í—Å–µ–≥–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ: {len(predictions)} / {len(inputs)}")

    # BERTScore
    print("Calculating BERTScore...")
    _, _, F1 = bert_score(predictions, references, lang="ru", model_type="xlm-roberta-large")
    bert_f1 = float(F1.mean())

    # ChrF
    print("Calculating ChrF...")
    chrf = CHRF()
    chrf_score = chrf.corpus_score(predictions, [references]).score

    # NER retention
    print("Calculating NER Retention...")
    total_ents, preserved_ents = 0, 0
    for src, pred in tqdm(zip(inputs, predictions), total=len(predictions), desc="NER Matching"):
        ents_src = extract_named_entities(src)
        ents_pred = extract_named_entities(pred)
        total_ents += len(ents_src)
        preserved_ents += len(ents_src & ents_pred)
    ner_retention = preserved_ents / total_ents if total_ents > 0 else 1.0

    # Toxicity
    print("Evaluating Toxicity...")
    toxicity_pipe = pipeline(
        "text-classification",
        model="s-nlp/russian_toxicity_classifier",
        tokenizer="s-nlp/russian_toxicity_classifier",
        top_k=None
    )

    scores = []
    for pred in tqdm(predictions, desc="‚ö†Ô∏è Evaluating toxicity"):
        result = toxicity_pipe(pred)[0]
        toxic_score = next((x['score'] for x in result if x['label'] == 'toxic'), 0.0)
        scores.append(toxic_score)

    avg_toxicity = sum(scores) / len(scores)

    # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã
    return {
        "bert_f1": round(bert_f1, 4),
        "chrf": round(chrf_score, 2),
        "ner_retention": round(ner_retention, 4),
        "avg_toxicity": round(avg_toxicity, 4)
    }


def show_examples(model, tokenizer, prompts, max_tokens=128):
    print("-" * 60)
    print("–ü—Ä–∏–º–µ—Ä—ã –¥–µ—Ç–æ–∫—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏:")
    for i, text in enumerate(prompts, 1):
        inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True)
        inputs = {k: v.to(model.device) for k, v in inputs.items()}
        with torch.no_grad():
            output_ids = model.generate(
                input_ids=inputs["input_ids"],
                attention_mask=inputs["attention_mask"],
                max_new_tokens=max_tokens
            )
        output = tokenizer.decode(output_ids[0], skip_special_tokens=True)
        print(f"{i}. TOXIC: {text}\n   DETOX:   {output}\n")


candidates = {
    "ruT5-base": 'ai-forever/ruT5-base',
    "detox": 's-nlp/ruT5-base-detox',
    "paraphraser": 'cointegrated/rut5-base-paraphraser',
    "mt5-base": 'google/mt5-base',
    "mbart-detox": "s-nlp/mbart-detox-en-ru",
    "rut5-detox-v2": "orzhan/rut5-base-detox-v2",
    "mbart-multilingual-detox": "textdetox/mbart-detox-baseline",
    "ruT5-base-detox-polite (–Ω–æ–≤–∞—è –º–æ–¥–µ–ª—å)": "ruT5-base-detox-polite",
    "ruT5-base-detox-polite-NER (–Ω–æ–≤–∞—è –º–æ–¥–µ–ª—å —Å Natasha)": "ruT5-base-detox-polite-NER",
}

for name, path in candidates.items():
    if "mbart" in name:

        model = MBartForConditionalGeneration.from_pretrained(path).to(device)
        tokenizer = AutoTokenizer.from_pretrained("facebook/mbart-large-50")
        tokenizer.src_lang = "ru_RU"
        tokenizer.tgt_lang = "ru_RU"

        print(f"üîé –ú–æ–¥–µ–ª—å: {name}")
        metrics = evaluate_model(model, tokenizer, "test.jsonl", mbart=True)
        print(metrics)

        def show_examples_mbart(model, tokenizer, prompts, max_tokens=128):
            print("-" * 60)
            print(f"–ü—Ä–∏–º–µ—Ä—ã –¥–µ—Ç–æ–∫—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ ({name}):")
            for i, text in enumerate(prompts, 1):
                inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True).to(model.device)
                with torch.no_grad():
                    outputs = model.generate(
                        **inputs,
                        forced_bos_token_id=tokenizer.lang_code_to_id[tokenizer.tgt_lang],
                        max_length=max_tokens,
                        num_beams=4,
                        do_sample=True,
                        temperature=1.0,
                    )
                decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)
                print(f"{i}. TOXIC: {text}\n   DETOX:   {decoded}\n")


        show_examples_mbart(model, tokenizer, test_phrases)

    elif name == 'mt5-base':
        model = MT5ForConditionalGeneration.from_pretrained(path).to(device)
        tokenizer = AutoTokenizer.from_pretrained(path)
        print(f"üîé –ú–æ–¥–µ–ª—å: {name}")
        metrics = evaluate_model(model, tokenizer, "test.jsonl")
        print(metrics)
        show_examples(model, tokenizer, list(map(add_para, test_phrases)))

    else:
        model = T5ForConditionalGeneration.from_pretrained(path).to(device)
        if '–Ω–æ–≤–∞—è' in name:
            tokenizer = AutoTokenizer.from_pretrained('ai-forever/ruT5-base')
        else:
            tokenizer = AutoTokenizer.from_pretrained(path)
        print(f"üîé –ú–æ–¥–µ–ª—å: {name}")
        metrics = evaluate_model(model, tokenizer, "test.jsonl")
        print(metrics)
        show_examples(model, tokenizer, test_phrases)
    print()




